{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Scatter Plot Evaluation\n",
    "\n",
    "This separate file is required to ensure that both models receive the same batches and genes so that comparability is possible.\n",
    "For this reason, both models are loaded and set to evaluation mode.\n",
    "\n",
    "<br>\n",
    "\n",
    "### File Requirements\n",
    "\n",
    "The following files are required:\n",
    "1. [`data/STEP00/Cell_line_RMA_proc_basalExp.txt`](https://www.cancerrxgene.org/gdsc1000/GDSC1000_WebResources/Home.html)\n",
    "2. `models/CCL_AUTOENCODER.pth` - obtained by STEP02_AUTOENCODER.ipynb\n",
    "3. `models/CCL_TRANSFORMER.pth` - obtained by STEP02_TRANSFORMER.ipynb\n",
    "\n",
    "<br>\n",
    "\n",
    "### Output\n",
    "A pdf file containing the scatter plots.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a478c9717aee8cc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.random.seed(42)  # Fixed seed for reproducibility\n",
    "torch.manual_seed(42)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2933a8835e5bfbba"
  },
  {
   "cell_type": "code",
   "source": [
    "ccl_file = pd.read_csv(\"data/STEP00/Cell_line_RMA_proc_basalExp.txt\", sep=\"\\t\")\n",
    "\n",
    "# Select data from row 1 (index 1) onwards and column 2 (index 2) onwards\n",
    "ccl_df = ccl_file.iloc[:, 2:].T\n",
    "\n",
    "# Drop duplicates and convert to a list\n",
    "rna_values = ccl_df.drop_duplicates()\n",
    "rna_values = rna_values.values.tolist()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3eee00f07920aa57",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Transformer Encoder Architecture\n",
    "\n",
    "The “CCL_TRANSFORMER.pth” model is loaded and set to evaluation mode."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "da3691503e450ed8"
  },
  {
   "cell_type": "code",
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "        self.qkv_proj = nn.Linear(embed_dim, embed_dim * 3)  \n",
    "        self.o_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length, embed_dim = x.size()\n",
    "        qkv = self.qkv_proj(x)\n",
    "        qkv = qkv.view(batch_size, seq_length, self.num_heads, 3 * self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1)\n",
    "        q, k, v = qkv.chunk(3, dim=2)\n",
    "        attn_scores = torch.matmul(q.transpose(-1, -2), k) / self.head_dim**0.5\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        attn_output = torch.matmul(attn_weights, v.transpose(-1, -2)).transpose(-1, -2)\n",
    "        attn_output = attn_output.contiguous().view(batch_size, seq_length, embed_dim)\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "        attn_output = self.dropout(attn_output)\n",
    "        \n",
    "        return attn_output\n",
    "\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_hidden_dim, dropout):\n",
    "        super(TransformerEncoderBlock, self).__init__()\n",
    "        self.attention = MultiHeadAttention(embed_dim, num_heads, dropout)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_hidden_dim, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output = self.attention(x)\n",
    "        x = self.norm1(x + attn_output)\n",
    "        ff_output = self.ff(x)\n",
    "        x = self.norm2(x + ff_output)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, num_heads, ff_hidden_dim, num_layers, dropout=0.1):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, embed_dim)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderBlock(embed_dim, num_heads, ff_hidden_dim, dropout)\n",
    "        for _ in range(num_layers)])\n",
    "        self.fc_out = nn.Linear(embed_dim, input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.embedding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.fc_out(x)\n",
    "        x = x.squeeze(1)\n",
    "\n",
    "        return x\n",
    "    \n",
    "transformer_model = TransformerEncoder(17737, 512, 8, 1024, 4, 0.5)\n",
    "transformer_model.load_state_dict(torch.load(\"models/CCL_TRANSFORMER.pth\"))\n",
    "transformer_model.eval()  # Set the model to evaluation mode"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "92c5e85a358f3d24",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Autoencoder Architecture\n",
    "The “CCL_AUTOENCODER.pth” model is loaded here and set to evaluation mode."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5729b25f0cdad352"
  },
  {
   "cell_type": "code",
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, latent_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Encoder\n",
    "        encoder_layers = []\n",
    "        dims = [input_dim] + hidden_dims\n",
    "        for i in range(len(dims) - 1):\n",
    "            encoder_layers.append(nn.Linear(dims[i], dims[i + 1]))\n",
    "            encoder_layers.append(nn.ReLU())\n",
    "        self.encoder = nn.Sequential(*encoder_layers)\n",
    "        self.latent = nn.Linear(hidden_dims[-1], latent_dim)\n",
    "\n",
    "        # Decoder\n",
    "        decoder_layers = []\n",
    "        dims = [latent_dim] + list(reversed(hidden_dims))\n",
    "        for i in range(len(dims) - 1):\n",
    "            decoder_layers.append(nn.Linear(dims[i], dims[i + 1]))\n",
    "            decoder_layers.append(nn.ReLU())\n",
    "        self.decoder = nn.Sequential(*decoder_layers)\n",
    "        self.output_layer = nn.Linear(hidden_dims[0], input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.latent(x)\n",
    "        x = self.decoder(x)\n",
    "        x = self.output_layer(x)  # No activation for reconstruction\n",
    "        return x\n",
    "\n",
    "autoencoder_model = Autoencoder(17737, [512, 128], 128)\n",
    "autoencoder_model.load_state_dict(torch.load(\"models/CCL_AUTOENCODER.pth\"))\n",
    "autoencoder_model.eval()  # Set the model to evaluation mode"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7266c0caec67bcb5",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6eb55789640b09bd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def scatter_plot(model1, model2, data_loader, device, num_genes=100, highlight_gene_indices=None):\n",
    "    model1.eval()\n",
    "    model2.eval()\n",
    "    model1.to(device)\n",
    "    model2.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Select same batch for both models\n",
    "        for batch in data_loader:\n",
    "            x_batch = batch[0].to(device)  # Move input batch to the correct device\n",
    "            reconstructed1 = model1(x_batch)  # Get reconstruction for model1\n",
    "            reconstructed2 = model2(x_batch)  # Get reconstruction for model2\n",
    "            x_original = x_batch.cpu().numpy()  # Move to CPU for numpy operations\n",
    "            x_reconstructed1 = reconstructed1.cpu().numpy()\n",
    "            x_reconstructed2 = reconstructed2.cpu().numpy()\n",
    "\n",
    "            break  # Use the first batch for visualization\n",
    "\n",
    "    # Select the same random subset of genes for both models to ensure comparability\n",
    "    gene_indices = np.random.choice(x_original.shape[1], num_genes, replace=False)\n",
    "    if highlight_gene_indices is not None:\n",
    "        for gene_idx in highlight_gene_indices:\n",
    "            if gene_idx not in gene_indices:\n",
    "                gene_indices = np.append(gene_indices, gene_idx)\n",
    "\n",
    "    # Assign colors to highlighted genes\n",
    "    highlight_colors = cm.Dark2(range(len(highlight_gene_indices))) \n",
    "    highlighted_gene_colors = {gene: color for gene, color in zip(highlight_gene_indices, highlight_colors)}\n",
    "\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    \n",
    "    # Autoencoder subplot\n",
    "    plt.subplot(1, 2, 1) \n",
    "    for i, gene_idx in enumerate(gene_indices):\n",
    "        if gene_idx in highlighted_gene_colors:\n",
    "            color = highlighted_gene_colors[gene_idx]\n",
    "            alpha = 1.0\n",
    "            label = f\"Gene {gene_idx}\"\n",
    "        else:\n",
    "            color = \"darkgrey\"\n",
    "            alpha = 1\n",
    "            label = None\n",
    "\n",
    "        plt.scatter(x_original[:, gene_idx], x_reconstructed1[:, gene_idx], \n",
    "                    alpha=alpha, label=label, color=color)\n",
    "\n",
    "    if highlight_gene_indices is not None:\n",
    "        plt.legend(title=\"Highlighted Genes\", fontsize=12, loc=\"upper left\")\n",
    "\n",
    "    plt.xlabel(\"Original Gene Expression Value\")\n",
    "    plt.ylabel(\"Reconstructed Gene Expression Value\")\n",
    "    plt.xlim(0, 14)\n",
    "    plt.ylim(0, 14)\n",
    "    plt.title(f\"{model1.__class__.__name__}\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Transformer subplot\n",
    "    plt.subplot(1, 2, 2) \n",
    "    for i, gene_idx in enumerate(gene_indices):\n",
    "        if gene_idx in highlighted_gene_colors:\n",
    "            color = highlighted_gene_colors[gene_idx]\n",
    "            alpha = 1.0\n",
    "            label = f\"Gene {gene_idx}\"\n",
    "        else:\n",
    "            color = \"darkgrey\"\n",
    "            alpha = 1\n",
    "            label = None\n",
    "\n",
    "        plt.scatter(x_original[:, gene_idx], x_reconstructed2[:, gene_idx], \n",
    "                    alpha=alpha, label=label, color=color)\n",
    "\n",
    "    if highlight_gene_indices is not None:\n",
    "        plt.legend(title=\"Highlighted Genes\", fontsize=12, loc=\"upper left\")\n",
    "\n",
    "    plt.xlabel(\"Original Gene Expression Value\")\n",
    "    plt.ylabel(\"Reconstructed Gene Expression Value\")\n",
    "    plt.xlim(0, 14)\n",
    "    plt.ylim(0, 14)\n",
    "    plt.title(f\"{model2.__class__.__name__}\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"comparison_{model1.__class__.__name__}_{model2.__class__.__name__}_scatter.pdf\", format=\"pdf\", bbox_inches=\"tight\") \n",
    "    plt.show()"
   ],
   "id": "30bee27c2bcb1ee0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create training and validation splits\n",
    "data = torch.tensor(rna_values, dtype=torch.float32)\n",
    "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "val_dataset = TensorDataset(val_data)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "scatter_plot(autoencoder_model, transformer_model, val_loader, device, num_genes=100, highlight_gene_indices=[1, 2, 3])"
   ],
   "id": "de8779df85606b4",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
