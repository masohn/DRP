{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Cancer Cell Line Feature Extraction with an Autoencoder\n",
    "\n",
    "In this Jupyter notebook, an Autoencoder is used to obtain cancer cell line feature representations.\n",
    "\n",
    "<br>\n",
    "\n",
    "### File Requirements\n",
    "\n",
    "The following file is required in the `data/STEP00` folder:\n",
    "\n",
    "1. [Cell_line_RMA_proc_basalExp.txt](https://www.cancerrxgene.org/gdsc1000/GDSC1000_WebResources/Home.html)\n",
    "\n",
    "<br>\n",
    "\n",
    "### Output\n",
    "The trained model \"CCL_AUTOENCODER.pth\".\n",
    "\n",
    "<br>\n",
    "\n",
    "### Evaluation\n",
    "Visualization of the learning curve and output of performance metrics.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "da5dd806eb93d023"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from scipy.stats import pearsonr\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "initial_id"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "np.random.seed(42)  # Fixed seed for reproducibility\n",
    "torch.manual_seed(42)"
   ],
   "id": "c7cf8bccdbf6910b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "ccl_file = pd.read_csv(\"data/STEP00/Cell_line_RMA_proc_basalExp.txt\", sep=\"\\t\")\n",
    "\n",
    "# Select data from row 1 (index 1) onwards and column 2 (index 2) onwards\n",
    "ccl_df = ccl_file.iloc[:, 2:].T\n",
    "\n",
    "# Drop duplicates and convert to a list\n",
    "rna_values = ccl_df.drop_duplicates()\n",
    "rna_values = rna_values.values.tolist()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3eee00f07920aa57",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Autoencoder Architecture"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e272abf55deb7093"
  },
  {
   "cell_type": "code",
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, latent_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Encoder\n",
    "        encoder_layers = []\n",
    "        dims = [input_dim] + hidden_dims\n",
    "        for i in range(len(dims) - 1):\n",
    "            encoder_layers.append(nn.Linear(dims[i], dims[i + 1]))\n",
    "            encoder_layers.append(nn.ReLU())\n",
    "        self.encoder = nn.Sequential(*encoder_layers)\n",
    "        \n",
    "        # Code (latent representation)\n",
    "        self.latent = nn.Linear(hidden_dims[-1], latent_dim)\n",
    "\n",
    "        # Decoder\n",
    "        decoder_layers = []\n",
    "        dims = [latent_dim] + list(reversed(hidden_dims))\n",
    "        for i in range(len(dims) - 1):\n",
    "            decoder_layers.append(nn.Linear(dims[i], dims[i + 1]))\n",
    "            decoder_layers.append(nn.ReLU())\n",
    "        self.decoder = nn.Sequential(*decoder_layers)\n",
    "        self.output_layer = nn.Linear(hidden_dims[0], input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.latent(x)\n",
    "        x = self.decoder(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "71080ea3293a2a23",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "188918178d3c4003"
  },
  {
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create training and validation splits\n",
    "data = torch.tensor(rna_values, dtype=torch.float32)\n",
    "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "train_dataset = TensorDataset(train_data)\n",
    "val_dataset = TensorDataset(val_data)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Hyperparameters\n",
    "input_dim = 17737   # Number of genes expression values\n",
    "hidden_dims = [512, 128]\n",
    "latent_dim = 128\n",
    "lr = 0.0006\n",
    "epochs=300\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 20  # Number of epochs to wait for improvement\n",
    "min_delta = 1e-4  # Minimum change to qualify as an improvement\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Model definition\n",
    "model = Autoencoder(input_dim, hidden_dims, latent_dim).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_train_loss = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        x_batch = batch[0].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(x_batch)\n",
    "        loss = criterion(outputs, x_batch)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_train_loss += loss.item()\n",
    "        \n",
    "    epoch_train_loss /= len(train_loader)\n",
    "    train_losses.append(epoch_train_loss)\n",
    "\n",
    "    # Validation loss\n",
    "    model.eval()\n",
    "    epoch_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            x_batch = batch[0].to(device)\n",
    "            outputs = model(x_batch)\n",
    "            loss = criterion(outputs, x_batch)\n",
    "            epoch_val_loss += loss.item()\n",
    "    epoch_val_loss /= len(val_loader)\n",
    "    val_losses.append(epoch_val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Training Loss: {epoch_train_loss:.4f}, Validation Loss: {epoch_val_loss:.4f}\")\n",
    "\n",
    "    # Early stopping\n",
    "    if epoch_val_loss < best_val_loss - min_delta:\n",
    "        best_val_loss = epoch_val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), \"models/CCL_AUTOENCODER.pth\")  # Save the best model\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if patience_counter >= patience:\n",
    "        print(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
    "        break\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "92c5e85a358f3d24",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "torch.save(model.state_dict(), \"models/CCL_AUTOENCODER.pth\")"
   ],
   "id": "90af000c7a8504b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation: Learning curve and performance metrics"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "238dc5300885f702"
  },
  {
   "cell_type": "code",
   "source": [
    "plt.style.use(\"seaborn-v0_8-ticks\")\n",
    "plt.rc(\"font\", family=\"Times New Roman\", size=12)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.tick_params(axis=\"both\", which=\"both\", direction=\"in\", length=6, width=1)\n",
    "\n",
    "ax.plot(train_losses, label=\"Training Loss\", linewidth=1.5)\n",
    "ax.plot(val_losses, label=\"Validation Loss\", linewidth=1.5)\n",
    "\n",
    "ax.set_xlabel(\"Training Epochs\")\n",
    "ax.set_ylabel(\"Mean Squared Error Loss\")\n",
    "ax.set_ylim(0.15, 0.8)\n",
    "ax.legend()\n",
    "\n",
    "plt.title(\"Autoencoder\")\n",
    "fig.savefig(\"autoencoder_loss.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5b87d3866b79e02c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def calculate_mse(model, data_loader, device):\n",
    "    model.eval()\n",
    "    mse_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            x_batch = batch[0].to(device)\n",
    "            reconstructed = model(x_batch)\n",
    "            mse = mean_squared_error(x_batch.cpu().numpy(), reconstructed.cpu().numpy())\n",
    "            mse_list.append(mse)\n",
    "    return np.mean(mse_list)\n",
    "\n",
    "\n",
    "def calculate_mae(model, data_loader, device):\n",
    "    model.eval()\n",
    "    mae_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            x_batch = batch[0].to(device)\n",
    "            reconstructed = model(x_batch)\n",
    "            mae = mean_absolute_error(x_batch.cpu().numpy(), reconstructed.cpu().numpy())\n",
    "            mae_list.append(mae)\n",
    "    return np.mean(mae_list)\n",
    "\n",
    "\n",
    "def calculate_pcc(model, data_loader, device):\n",
    "    model.eval()\n",
    "    pcc_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            x_batch = batch[0].to(device)\n",
    "            reconstructed = model(x_batch)\n",
    "            x_original = x_batch.cpu().numpy()\n",
    "            x_reconstructed = reconstructed.cpu().numpy()\n",
    "            \n",
    "            # Calculate Pearson correlation coefficient for each gene\n",
    "            for gene_idx in range(x_original.shape[1]):\n",
    "                r, _ = pearsonr(x_original[:, gene_idx], x_reconstructed[:, gene_idx])\n",
    "                pcc_list.append(r)\n",
    "    return np.mean(pcc_list)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b87b1018c9321b9a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f\"Training MSE: {calculate_mse(model, train_loader, device):.4f}\")\n",
    "print(f\"Validation MSE: {calculate_mse(model, val_loader, device):.4f}\")\n",
    "print(f\"Training RMSE: {math.sqrt(calculate_mse(model, train_loader, device)):.4f}\")\n",
    "print(f\"Validation RMSE: {math.sqrt(calculate_mse(model, val_loader, device)):.4f}\")\n",
    "print(f\"Training MAE: {calculate_mae(model, train_loader, device):.4f}\")\n",
    "print(f\"Validation MAE: {calculate_mae(model, val_loader, device):.4f}\")\n",
    "print(f\"Training PCC: {calculate_pcc(model, train_loader, device):.4f}\")\n",
    "print(f\"Validation PCC: {calculate_pcc(model, val_loader, device):.4f}\")"
   ],
   "id": "cb970026cca21e83",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "5160012f66e183b0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
