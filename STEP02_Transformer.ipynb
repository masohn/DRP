{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Cancer Cell Line Feature Extraction with a Transformer Encoder\n",
    "\n",
    "In this Jupyter notebook, a Transformer Encoder is used to obtain cancer cell line feature representations.\n",
    "\n",
    "<br>\n",
    "\n",
    "### File Requirements\n",
    "\n",
    "The following file is required in the `data/STEP00` folder:\n",
    "\n",
    "1. [Cell_line_RMA_proc_basalExp.txt](https://www.cancerrxgene.org/gdsc1000/GDSC1000_WebResources/Home.html)\n",
    "\n",
    "<br>\n",
    "\n",
    "### Output\n",
    "The trained model \"CCL_TRANSFORMER.pth\".\n",
    "\n",
    "<br>\n",
    "\n",
    "### Evaluation\n",
    "Visualization of the learning curve and output of performance metrics.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f8c74a7ac01b3df"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from scipy.stats import pearsonr"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "source": [
    "np.random.seed(42)  # Fixed seed for reproducibility\n",
    "torch.manual_seed(42)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3eee00f07920aa57",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "ccl_file = pd.read_csv(\"data/STEP00/Cell_line_RMA_proc_basalExp.txt\", sep=\"\\t\")\n",
    "\n",
    "# Select data from row 1 (index 1) onwards and column 2 (index 2) onwards\n",
    "ccl_df = ccl_file.iloc[:, 2:].T\n",
    "\n",
    "# Drop duplicates and convert to a list\n",
    "rna_values = ccl_df.drop_duplicates()\n",
    "rna_values = rna_values.values.tolist()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eff4d231e28d2fe6",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Transformer Encoder\n",
    "PyTorch built-in modules are feasible but the scratch implementation yielded better results, most likely due to the MultiHeadAttention module."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cac5080726d1bdfa"
  },
  {
   "cell_type": "code",
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        # Ensure that the embedding dimension is divisible by the number of heads\n",
    "        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "        \n",
    "        # Linear projection to compute the query, key, and value for attention\n",
    "        self.qkv_proj = nn.Linear(embed_dim, embed_dim * 3)  # Projects input to 3 * embed_dim (query, key, value)\n",
    "        \n",
    "        # Linear projection to map the attention output back to the embedding dimension\n",
    "        self.o_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        # Dropout layer to apply regularization during training\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Get the batch size, sequence length, and embedding dimension from input\n",
    "        batch_size, seq_length, embed_dim = x.size()\n",
    "        \n",
    "        # Project the input to query, key, and value vectors using the qkv projection\n",
    "        qkv = self.qkv_proj(x)\n",
    "        \n",
    "        # Reshape and permute the projected qkv to separate heads\n",
    "        # The shape of qkv becomes (batch_size, seq_length, num_heads, 3 * head_dim)\n",
    "        qkv = qkv.view(batch_size, seq_length, self.num_heads, 3 * self.head_dim)\n",
    "        \n",
    "        # Permute to get the shape (num_heads, batch_size, 3 * head_dim, seq_length)\n",
    "        qkv = qkv.permute(2, 0, 3, 1)\n",
    "        \n",
    "        # Split the qkv tensor into individual query, key, and value tensors\n",
    "        q, k, v = qkv.chunk(3, dim=2)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        attn_scores = torch.matmul(q.transpose(-1, -2), k) / self.head_dim**0.5\n",
    "        \n",
    "        # Apply softmax to the attention scores to get attention weights\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        \n",
    "        # Compute the attention output by multiplying the attention weights with values\n",
    "        attn_output = torch.matmul(attn_weights, v.transpose(-1, -2)).transpose(-1, -2)\n",
    "        \n",
    "        # Reshape the attention output to match the input shape (batch_size, seq_length, embed_dim)\n",
    "        attn_output = attn_output.contiguous().view(batch_size, seq_length, embed_dim)\n",
    "        \n",
    "        # Project the attention output back to the embedding dimension\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "        \n",
    "        # Apply dropout regularization to the output\n",
    "        attn_output = self.dropout(attn_output)\n",
    "        \n",
    "        return attn_output\n",
    "\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_hidden_dim, dropout=0.1):\n",
    "        super(TransformerEncoderBlock, self).__init__()\n",
    "        \n",
    "        # Multi-head attention layer\n",
    "        self.attention = MultiHeadAttention(embed_dim, num_heads, dropout)\n",
    "        \n",
    "        # Layer normalization \n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # Feed-forward network with hidden layer size ff_hidden_dim\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_hidden_dim, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply multi-head attention to input\n",
    "        attn_output = self.attention(x)\n",
    "        \n",
    "        # Residual connection: Add input and attention output, then normalize\n",
    "        x = self.norm1(x + attn_output)\n",
    "        \n",
    "        # Apply feed-forward network\n",
    "        ff_output = self.ff(x)\n",
    "        \n",
    "        # Residual connection: Add input and feed-forward output, then normalize\n",
    "        x = self.norm2(x + ff_output)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, num_heads, ff_hidden_dim, num_layers, dropout=0.1):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        \n",
    "        # Linear layer to convert input dimension to embedding dimension\n",
    "        self.embedding = nn.Linear(input_dim, embed_dim)\n",
    "        \n",
    "        # Stack of transformer encoder blocks\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderBlock(embed_dim, num_heads, ff_hidden_dim, dropout)\n",
    "        for _ in range(num_layers)])\n",
    "        \n",
    "        # Output linear layer to project embedding back to input dimension\n",
    "        self.fc_out = nn.Linear(embed_dim, input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Add sequence length dimension\n",
    "        x = x.unsqueeze(1)\n",
    "        \n",
    "        # Apply embedding layer\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # Pass through all transformer encoder blocks\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        # Apply output linear layer to get final result\n",
    "        x = self.fc_out(x)\n",
    "        \n",
    "        # Remove sequence length dimension\n",
    "        x = x.squeeze(1)\n",
    "        \n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "71080ea3293a2a23",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "384dcc9efd6c4c04"
  },
  {
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create training and validation splits\n",
    "data = torch.tensor(rna_values, dtype=torch.float32)\n",
    "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "train_dataset = TensorDataset(train_data)\n",
    "val_dataset = TensorDataset(val_data)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Hyperparameters\n",
    "input_dim = 17737   # Number of genes expression values\n",
    "embed_dim = 512\n",
    "num_heads = 8\n",
    "ff_hidden_dim = 1024\n",
    "num_layers = 4\n",
    "dropout = 0.5\n",
    "epochs = 300\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 20  # Number of epochs to wait for improvement\n",
    "min_delta = 1e-4  # Minimum change to qualify as an improvement\n",
    "best_val_loss = float(\"inf\")\n",
    "patience_counter = 0\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Model definition\n",
    "model = TransformerEncoder(input_dim, embed_dim, num_heads, ff_hidden_dim, num_layers, dropout).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0008752047398730367)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_train_loss = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        x_batch = batch[0].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(x_batch)\n",
    "        loss = criterion(outputs, x_batch)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_train_loss += loss.item()\n",
    "        \n",
    "    epoch_train_loss /= len(train_loader)\n",
    "    train_losses.append(epoch_train_loss)\n",
    "\n",
    "    # Validation loss\n",
    "    model.eval()\n",
    "    epoch_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            x_batch = batch[0].to(device)\n",
    "            outputs = model(x_batch)\n",
    "            loss = criterion(outputs, x_batch)\n",
    "            epoch_val_loss += loss.item()\n",
    "    epoch_val_loss /= len(val_loader)\n",
    "    val_losses.append(epoch_val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Training Loss: {epoch_train_loss:.4f}, Validation Loss: {epoch_val_loss:.4f}\")\n",
    "\n",
    "    # Early stopping\n",
    "    if epoch_val_loss < best_val_loss - min_delta:\n",
    "        best_val_loss = epoch_val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), \"models/CCL_TRANSFORMER.pth\")  # Save the best model\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if patience_counter >= patience:\n",
    "        print(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
    "        break\n",
    "\n",
    "print(\"Training complete\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "92c5e85a358f3d24",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "torch.save(model.state_dict(), \"models/CCL_TRANSFORMER.pth\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7266c0caec67bcb5",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation: Learning curve and performance metrics"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bb902b9a500a1da8"
  },
  {
   "cell_type": "code",
   "source": [
    "#plt.style.use(\"classic\")\n",
    "plt.style.use(\"seaborn-v0_8-ticks\")\n",
    "plt.rc(\"font\", family=\"Times New Roman\", size=12)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.tick_params(axis=\"both\", which=\"both\", direction=\"in\", length=6, width=1)\n",
    "\n",
    "ax.plot(train_losses, label=\"Training Loss\", linewidth=1.5)\n",
    "ax.plot(val_losses, label=\"Validation Loss\", linewidth=1.5)\n",
    "\n",
    "\n",
    "ax.set_xlabel(\"Training Epochs\")\n",
    "ax.set_ylabel(\"Mean Squared Error Loss\")\n",
    "ax.set_ylim(0.15, 0.8)\n",
    "ax.legend()\n",
    "\n",
    "plt.title(\"TransformerEncoder\")\n",
    "fig.savefig(\"transformer_loss.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5b87d3866b79e02c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def calculate_mse(model, data_loader, device):\n",
    "    model.eval()\n",
    "    mse_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            x_batch = batch[0].to(device)\n",
    "            reconstructed = model(x_batch)\n",
    "            mse = mean_squared_error(x_batch.cpu().numpy(), reconstructed.cpu().numpy())\n",
    "            mse_list.append(mse)\n",
    "    return np.mean(mse_list)\n",
    "\n",
    "\n",
    "def calculate_mae(model, data_loader, device):\n",
    "    model.eval()\n",
    "    mae_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            x_batch = batch[0].to(device)\n",
    "            reconstructed = model(x_batch)\n",
    "            mae = mean_absolute_error(x_batch.cpu().numpy(), reconstructed.cpu().numpy())\n",
    "            mae_list.append(mae)\n",
    "    return np.mean(mae_list)\n",
    "\n",
    "\n",
    "def calculate_pcc(model, data_loader, device):\n",
    "    model.eval()\n",
    "    pcc_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            x_batch = batch[0].to(device)\n",
    "            reconstructed = model(x_batch)\n",
    "            x_original = x_batch.cpu().numpy()\n",
    "            x_reconstructed = reconstructed.cpu().numpy()\n",
    "            \n",
    "            # Calculate Pearson correlation coefficient for each gene\n",
    "            for gene_idx in range(x_original.shape[1]):\n",
    "                r, _ = pearsonr(x_original[:, gene_idx], x_reconstructed[:, gene_idx])\n",
    "                pcc_list.append(r)\n",
    "    return np.mean(pcc_list)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b87b1018c9321b9a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f\"Training MSE: {calculate_mse(model, train_loader, device):.4f}\")\n",
    "print(f\"Validation MSE: {calculate_mse(model, val_loader, device):.4f}\")\n",
    "print(f\"Training RMSE: {math.sqrt(calculate_mse(model, train_loader, device)):.4f}\")\n",
    "print(f\"Validation RMSE: {math.sqrt(calculate_mse(model, val_loader, device)):.4f}\")\n",
    "print(f\"Training MAE: {calculate_mae(model, train_loader, device):.4f}\")\n",
    "print(f\"Validation MAE: {calculate_mae(model, val_loader, device):.4f}\")\n",
    "print(f\"Training PCC: {calculate_pcc(model, train_loader, device):.4f}\")\n",
    "print(f\"Validation PCC: {calculate_pcc(model, val_loader, device):.4f}\")"
   ],
   "id": "f7a1535e3856fba",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "bccb0f8a172429e3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
